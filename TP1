1.2 () Write down the set of coupled linear equations, analogous to (1.122), satisfied
by the coefficients wi which minimize the regularized sum-of-squares error function
given by (1.4)

link :https://rpubs.com/rpizarro/593184#:~:text=La%20Regresi%C3%B3n%20Polinomial%20es%20un,utiliza%20tres%20variables%2C%20como%20predictores.
      https://www.pluralsight.com/guides/linear-lasso-and-ridge-regression-with-r
library(MASS)
library(tidyverse) 
library(ggplot2)
library(knitr)
data("Boston")
datos <- Boston

##Use esta pagina para obtener un modelo random 
#entreno el modelo
set.seed(100) 

index = sample(1:nrow(datos), 0.7*nrow(datos)) 

train = datos[index,] # Create the training data 
test = datos[-index,] # Create the test data

dim(train)
dim(test)

cols = c('medv', 'lstat' )
library(caret)
pre_proc_val <- preProcess(train[,cols], method = c("center", "scale"))

train[,cols] = predict(pre_proc_val, train[,cols])
test[,cols] = predict(pre_proc_val, test[,cols])

summary(train)
#uso esos datos de entrenamiento para hacer el modelo polinomico

modelo.pol2 <- lm(formula = medv ~ lstat, data = datos) ##hago un polinomio de grado 9

coeficientes (w) = (22.532806, -152.459549,  64.227239, -27.051098, 25.451732 , -19.252418 , 6.508753 ,  1.941637, -6.729879, 8.416820 )

Regularizacion por medio de una ridge regression (suma de cuadrados de los coeficientes)

Trata de minimizar la suma de cuadrados de los residuos (https://www.statology.org/ridge-regression-in-r/)
              RSS = Σ(yi – ŷi)2
library(glmnet)

library(glmnet)
cols_reg = c('medv', 'lstat')

dummies <- dummyVars(medv ~ ., data = datos[,cols_reg])

train_dummies = predict(dummies, newdata = train[,cols_reg])

test_dummies = predict(dummies, newdata = test[,cols_reg])

print(dim(train_dummies)); print(dim(test_dummies))

library(glmnet)

x = as.matrix(train_dummies)
y_train = train$medv

x_test = as.matrix(test_dummies)
y_test = test$medv

lambdas <- 10^seq(2, -3, by = -.1)

error = 0.5*(sum (x - y_train)^2)+ lambdas/2*(sum(abs(w)^2)) ##regularized error function
error = 0
-0.5*(sum (x - y_train)^2) = lambdas/2*(sum(abs(w)^2))
suma_w = lambdas/(-0.5*(sum (x - y_train)^2))
coef = (abs(suma_w))^2

equation = w0 + w*xn 
1.3( ) Suppose that we have three coloured boxes r (red), b (blue), and g (green).
Box r contains 3 apples, 4 oranges, and 3 limes, box b contains 1 apple, 1 orange,
and 0 limes, and box g contains 3 apples, 3 oranges, and 4 limes. If a box is chosen
at random with probabilities p(r)=0.2, p(b)=0.2, p(g)=0.6, and a piece of
fruit is removed from the box (with equal probability of selecting any of the items in
the box), then what is the probability of selecting an apple? If we observe that the
selected fruit is in fact an orange, what is the probability that it came from the green
box?

p(apple) = p(apple/r)*p(r) + p(apple/b)*p(b) + p(apple/g)*p(g)
p(apple) = 3/22*0.2 + 1/22*0.2 + 3/22*0.6
p(apple) = 0.34

1.5 () Using the definition (1.38) show that var[f(x)] satisfies (1.39)

var[f] = E(f(x) − E[f(x)])2	
var[f] =  E(f(x))2 - 2E[f(x)*E(fx)]+(E[f(x)])2
var[f] = E(f(x))2  -  2E(f(x))*E(fx) +(E[f(x)])2
var[f] = E(f(x))2  -  2E[f(x)]2 + E[f(x)]2
var[f] = E(f(x))2  - E[f(x)]2


1.6 () Show that if two variables x and y are independent, then their covariance is
zero

ρXY= σXY/(σXσY)

Si las variables son independientes entonces σXY = 0 , es decir la probabilidad de variacion en conjunto es cero

1.11 () By setting the derivatives of the log likelihood function (1.54) with respect to µ
and σ2 equal to zero, verify the results (1.55) and (1.56).

ln2pi = 1.83787706641

ln(px|µ, σ2) = − 1/2σ2*suma(xn − µ)2 + N/2lnσ2 - N/2*ln2pi
0 = − 1/2σ2 (xn2 - 2xnu + u2) + Nlnσ2 - 1/2*N*1.83787706641


1.21 ( ) Consider two nonnegative numbers a and b, and show that, if a =< b, then
a =< sqrt((ab)). 

Use this result to show that, if the decision regions of a two-class
classification problem are chosen to minimize the probability of misclassification,
this probability will satisfy
a = 2 b = 5

a = raiz(10)
a = 3.16

p(mistake) =< raiz({p(x, C1)p(x, C2)})dx

Entonces si a es x en c1 y b es la probabilidad de x en c2 el producto de ambas aplicada la raiz siempre sera menor. 

1.30 ( ) Evaluate the Kullback-Leibler divergence (1.113) between two Gaussians
p(x) = N (x|µ, σ2) and q(x) = N (x|m, s2)

KL(p,q)=−∫p(x)logq(x)dx+∫p(x)logp(x)dx

1/2log(2πs2)+(σ2+(μ−m)2)/2s2−1/2*(1+log2πσ2)= 

logσ2/σ +(σ2+(μ−m)2)2s2−1/2

Si ambas son iguales la ultima linea se transformaa en cero, mientras si son distintas es distinto de cero (las distribuciones son distintas)

b. Para el caso de 1.30, generar simulaciones de la divergencia KL cuando la primera 
distribución está normalizada. Luego repetir intercambiando por la segunda.
Generar visualización de KL respecto de las varianzas y respecto de las media

#define two probability distributions
P = [.05, .1, .2, .05, .15, .25, .08, .12]
Q = [.3, .1, .2, .1, .1, .02, .08, .1]
S = P = [.05, .1, .2, .05, .15, .25, .08, .12]

from scipy.special import rel_entr

#calculate (P || Q)
sum(rel_entr(P, Q)) = 0.589885181619163
sum(rel_entr(Q, P)) = 0.497549319448034
sum(rel_entr(P,S)) = 0


###Segunda opcion mas automatica. Link: https://www.moonbooks.org/Articles/How-to-calculate-and-visualize-Kullback-Leibler-divergence-using-python-/
#divergencia KL entre dos distribuciones normales 

import matplotlib.pyplot as plt
import numpy as np
import matplotlib.mlab as mlab

from scipy.stats import norm
from scipy.integrate import quad

def p(x):
    return norm.pdf(x, 0, 2)

def q(x):
    return norm.pdf(x, 2, 2)

def KL(x):
    return p(x) * np.log( p(x) / q(x) )

range = np.arange(-10, 10, 0.001)

KL_int, err = quad(KL, -10, 10) 
print( 'KL: ', KL_int )

fig = plt.figure(figsize=(18, 8), dpi=100)

#---------- First Plot

ax = fig.add_subplot(1,2,1)
ax.grid(True)
ax.spines['left'].set_position('zero')
ax.spines['right'].set_color('none')
ax.spines['bottom'].set_position('zero')
ax.spines['top'].set_color('none')
ax.set_xlim(-10,10)
ax.set_ylim(-0.1,0.25)

ax.text(-2.5, 0.17, 'p(x)', horizontalalignment='center',fontsize=17)
ax.text(4.5, 0.17, 'q(x)', horizontalalignment='center',fontsize=17)

plt.plot(range, p(range))
plt.plot(range, q(range))

#divergencia entre una distribucion normal y no normal
def q(x):
    return norm.pdf(x, 5, 4)
range = np.arange(-10, 10, 0.001)

KL_int, err = quad(KL, -10, 10) 
print( 'KL: ', KL_int )
