1.2 () Write down the set of coupled linear equations, analogous to (1.122), satisfied
by the coefficients wi which minimize the regularized sum-of-squares error function
given by (1.4)

https://github.com/VirGonzalez/AprendAutomatico/issues/1#issue-1378319892

1.3( ) Suppose that we have three coloured boxes r (red), b (blue), and g (green).
Box r contains 3 apples, 4 oranges, and 3 limes, box b contains 1 apple, 1 orange,
and 0 limes, and box g contains 3 apples, 3 oranges, and 4 limes. If a box is chosen
at random with probabilities p(r)=0.2, p(b)=0.2, p(g)=0.6, and a piece of
fruit is removed from the box (with equal probability of selecting any of the items in
the box), then what is the probability of selecting an apple? If we observe that the
selected fruit is in fact an orange, what is the probability that it came from the green
box?
p(x) = p(x/r)*p(r) + p(x/b)*p(b) + p(x/g)*p(g)
p(apple) = p(apple/r)*p(r) + p(apple/b)*p(b) + p(apple/g)*p(g)
p(apple) = 3/22*0.2 + 1/22*0.2 + 3/22*0.6
p(apple) = 0.34

1.5 () Using the definition (1.38) show that var[f(x)] satisfies (1.39)

var[f] = E(f(x) − E[f(x)])2	
var[f] =  E(f(x))2 - 2E[f(x)*E(fx)]+(E[f(x)])2
var[f] = E(f(x))2  -  2E(f(x))*E(fx) +(E[f(x)])2
var[f] = E(f(x))2  -  2E[f(x)]2 + E[f(x)]2
var[f] = E(f(x))2  - E[f(x)]2


1.6 () Show that if two variables x and y are independent, then their covariance is
zero

ρXY= σXY/(σXσY)

Si las variables son independientes entonces σXY = 0 , es decir la probabilidad de variacion en conjunto es cero

1.11 () By setting the derivatives of the log likelihood function (1.54) with respect to µ
and σ2 equal to zero, verify the results (1.55) and (1.56).

https://github.com/VirGonzalez/AprendAutomatico/issues/1#issue-1378319892


1.21 ( ) Consider two nonnegative numbers a and b, and show that, if a =< b, then
a =< sqrt((ab)). 

Use this result to show that, if the decision regions of a two-class
classification problem are chosen to minimize the probability of misclassification,
this probability will satisfy
a = 2 b = 5

a = raiz(10)
a = 3.16

p(mistake) =< raiz({p(x, C1)p(x, C2)})dx

Entonces si a es x en c1 y b es la probabilidad de x en c2 el producto de ambas aplicada la raiz siempre sera menor. 

1.30 ( ) Evaluate the Kullback-Leibler divergence (1.113) between two Gaussians
p(x) = N (x|µ, σ2) and q(x) = N (x|m, s2)

KL(p,q)=−∫p(x)logq(x)dx+∫p(x)logp(x)dx

1/2log(2πs2)+(σ2+(μ−m)2)/2s2−1/2*(1+log2πσ2)= 

logσ2/σ +(σ2+(μ−m)2)2s2−1/2

Si ambas son iguales la ultima linea se transformaa en cero, mientras si son distintas es distinto de cero (las distribuciones son distintas)

b. Para el caso de 1.30, generar simulaciones de la divergencia KL cuando la primera 
distribución está normalizada. Luego repetir intercambiando por la segunda.
Generar visualización de KL respecto de las varianzas y respecto de las media

#define two probability distributions
P = [.05, .1, .2, .05, .15, .25, .08, .12]
Q = [.3, .1, .2, .1, .1, .02, .08, .1]
S = P = [.05, .1, .2, .05, .15, .25, .08, .12]

from scipy.special import rel_entr

#calculate (P || Q)
sum(rel_entr(P, Q)) = 0.589885181619163
sum(rel_entr(Q, P)) = 0.497549319448034
sum(rel_entr(P,S)) = 0


###Segunda opcion mas automatica. Link: https://www.moonbooks.org/Articles/How-to-calculate-and-visualize-Kullback-Leibler-divergence-using-python-/
#divergencia KL entre dos distribuciones normales 

import matplotlib.pyplot as plt
import numpy as np
import matplotlib.mlab as mlab

from scipy.stats import norm
from scipy.integrate import quad

def p(x):
    return norm.pdf(x, 0, 2)

def q(x):
    return norm.pdf(x, 2, 2)

def KL(x):
    return p(x) * np.log( p(x) / q(x) )

range = np.arange(-10, 10, 0.001)

KL_int, err = quad(KL, -10, 10) 
print( 'KL: ', KL_int ) = 0.4999997133484283

fig = plt.figure(figsize=(18, 8), dpi=100)

#---------- First Plot

ax = fig.add_subplot(1,2,1)
ax.grid(True)
ax.spines['left'].set_position('zero')
ax.spines['right'].set_color('none')
ax.spines['bottom'].set_position('zero')
ax.spines['top'].set_color('none')
ax.set_xlim(-10,10)
ax.set_ylim(-0.1,0.25)

ax.text(-2.5, 0.17, 'p(x)', horizontalalignment='center',fontsize=17)
ax.text(4.5, 0.17, 'q(x)', horizontalalignment='center',fontsize=17)

plt.plot(range, p(range))
plt.plot(range, q(range))

#divergencia entre una distribucion normal y no normal (segunda no normal)
def p(x):
    return norm.pdf(x, 2, 2)
    
def q(x):
    return norm.pdf(x, 5, 4)
range = np.arange(-10, 10, 0.001)

KL_int, err = quad(KL, -10, 10) 
print( 'KL: ', KL_int ) =  1.0994021254702657

#divergencia entre una distribucion primera no normal ç
def p(x):
    return norm.pdf(x, 5, 4)
    
def q(x):
    return norm.pdf(x, 2, 2)
    
range = np.arange(-10, 10, 0.001)

KL_int, err = quad(KL, -10, 10) 
print( 'KL: ', KL_int ) = 0.8362423421891381

https://github.com/VirGonzalez/AprendAutomatico/issues/1#issue-1378319892
